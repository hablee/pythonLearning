```python
import matplotlib.pyplot as plt
import numpy as np
import os
import PIL
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential

###================download dataset，下载数据集
import pathlib
dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
data_dir = pathlib.Path(data_dir)

image_count = len(list(data_dir.glob('*/*.jpg')))
print(image_count)

###==============展示几张图片，确保正确
# # show roses
# roses = list(data_dir.glob('roses/*'))
# img_show1=PIL.Image.open(str(roses[0]))
# # img_show = np.array(img_show)
# # print(img_show.shape)
# plt.imshow(img_show1)
# plt.show()

# img_show2=PIL.Image.open(str(roses[1]))
# img_show = np.array(img_show2)
# print(img_show.shape)
# plt.imshow(img_show2)
# plt.show()

# # show tulips
# tulips = list(data_dir.glob('tulips/*'))
# img1 = PIL.Image.open(str(tulips[0]))
# plt.imshow(img1)
# plt.show()
# img2 = PIL.Image.open(str(tulips[1]))
# plt.imshow(img2)
# plt.show()

####=======================create dataset，创建训练集和测试集
batch_size = 32
img_height = 180
img_width = 180

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset='training',
    seed=123,

    image_size=(img_height,img_width),
    batch_size=batch_size
)
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset='validation',
    seed=123,
    image_size=(img_height,img_width),
    batch_size=batch_size
)

class_names = train_ds.class_names
print(class_names)

###============visualize the data，可视化数据
# import matplotlib.pyplot as plt
# plt.figure(figsize=(10,10))
# for images,labels in train_ds.take(1):
#     for i in range(9):
#         ax = plt.subplot(3,3,i+1)
#         plt.imshow(images[i].numpy().astype("uint8"))
#         plt.title(class_names[labels[i]])
#         plt.axis('off')
#     plt.show()
#
# for image_batch,labels_batch in train_ds:
#     print(image_batch.shape)
#     print(labels_batch.shape)
#     break

###=============configure the dataset for performance，数据集配置
AUTOTUNE = tf.data.experimental.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

###============standardize the data
normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)

###============create model,创建模型
num_classes = 5

model = Sequential([
    layers.experimental.preprocessing.Rescaling(1./255,input_shape=(img_height,img_width,3)),
    layers.Conv2D(16,3,padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3,padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64,3,padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dense(128,activation='relu'),
    layers.Dense(num_classes)
])

###==============Compile the model，编译模型
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

###===============model summary
model.summary()

###===============train the model，训练模型
# epochs = 10
# history = model.fit(
#     train_ds,
#     validation_data=val_ds,
#     epochs=epochs,
# )

###=============visualize training results，可视化训练结果
# acc = history.history['accuracy']
# val_acc = history.history['val_accuracy']
#
# loss = history.history['loss']
# val_loss = history.history['val_loss']
#
# epochs_range = range(epochs)
#
# plt.figure(figsize=(8,8))
# plt.subplot(1,2,1)
# plt.plot(epochs_range,acc,label='Training Accuracy')
# plt.plot(epochs_range,val_acc,label='Validation Accuracy')
# plt.legend(loc='lower right')
# plt.title('Training and Validation Accuracy')

# plt.subplot(1,2,2)
# plt.plot(epochs_range,loss,label='Training Loss')
# plt.plot(epochs_range,val_loss,label='Validation Loss')
# plt.legend(loc='lower right')
# plt.title('Training and Validation Loss')
# plt.show()

##=========data augmentation,因为数据集不够，进行数据增强
data_augmentation = keras.Sequential(
    [
        layers.experimental.preprocessing.RandomFlip("horizontal",
                                                     input_shape=(img_height,
                                                     img_width,
                                                     3)),
        layers.experimental.preprocessing.RandomRotation(0.1),
        layers.experimental.preprocessing.RandomZoom(0.1),
    ]
)
# plt.figure(figsize=(10,10))
# for images,_ in train_ds.take(1):
#     for i in range(9):
#         augmented_images = data_augmentation(images)
#         print(augmented_images.shape)
#         ax = plt.subplot(3,3,i+1)
#         plt.imshow(augmented_images[0].numpy().astype("uint8"))
#         plt.axis("off")
#     plt.show()

###==================dropout，防止过拟合
model = Sequential([
    data_augmentation,
    layers.experimental.preprocessing.Rescaling(1./255),
    layers.Conv2D(16,3,padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32,3,padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64,3,padding='same',activation='relu'),
    layers.MaxPooling2D(),
    layers.Dropout(0.2),
    layers.Flatten(),
    layers.Dense(128,activation='relu'),
    layers.Dense(num_classes)
])

###===============compile and train the model，编译和训练模型
# model.compile(optimizer='adam',
#               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
#               metrics=['accuracy'])
#
# model.summary()
#
# epochs = 15
# history = model.fit(
#     train_ds,
#     validation_data=val_ds,
#     epochs=epochs,
# )

###===============保存模型
# model.save('model_imgclass.h5')


# ###============visualize training results，可视化训练结果
# acc = history.history['accuracy']
# val_acc = history.history['val_accuracy']
# loss = history.history['loss']
# val_loss = history.history['val_loss']
# epochs_range = range(epochs)
# plt.figure(figsize=(8,8))
# plt.subplot(1,2,1)
# plt.plot(epochs_range,acc,label='Training Accuracy')
# plt.plot(epochs_range,val_acc,label='Validation Accuracy')
# plt.legend(loc='lower right')
# plt.title('Training and Validation Accuracy')
# plt.subplot(1,2,2)
# plt.plot(epochs_range,acc,label='Training Loss')
# plt.plot(epochs_range,val_acc,label='Validation Loss')
# plt.legend(loc='lower right')
# plt.title('Training and Validation Loss')
# plt.show()

###================predict on new data，导入模型，预测新数据
from tensorflow.keras.models import load_model

model = load_model('model_imgclass.h5')

sunflower_url="https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg"
sunflower_path = tf.keras.utils.get_file('Red_sunflower',origin=sunflower_url)
img=keras.preprocessing.image.load_img(
    sunflower_path,target_size=(img_height,img_width)
)
img_array = keras.preprocessing.image.img_to_array(img)
img_array = tf.expand_dims(img_array,0) # create a batch
predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])
print("This image most likely belongs to {} with a {:.2f} percent confidence."
      .format(class_names[np.argmax(score)],100*np.max(score)))
```
